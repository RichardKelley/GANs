{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The premise behind _generative adversarial networks_ is that we can turn the process of learning a probability distribution into a game. The game has two players: a _generator_ and a _discriminator_. The starting point of the game is a _data distribution_ $p(x)$ that what we're trying to learn. The discriminator is a function $D$ that takes an input that may or may not be from the data distribution and decides whether or not the input is from $p(x)$. The generator is a function $G$ that takes random noise as its input and produces an output that can \"fool\" $D$ into classifying it as a real sample from $p(x)$. \n",
    "\n",
    "Both $D$ and $G$ are implemented as neural networks. The training process for these neural nets alternates between optimizing $D$ and optimizing $G$. At the end of the game, the hope is that $G$ has learned to produce such good outputs that $D$ can't distinguish them from real samples. This notebook walks through that process on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how GANs work, we'll use the MNIST dataset. Our generator will learn how to generate small pictures of handwritten digits, and our discriminator will try to distinguish between real images and fakes. Tensorflow makes working with MNIST pretty trivial. The only thing worth noting in the next cell is that we use the \"one hot\" encoding for our labels. That means that if the label for our image is 4, then the representation of the label would be a vector of length 10 that contains zeros everywhere except in the fifth position (assuming we put 0 at the beginning of the vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two parameters that directly impact training time and model quality: `n_train_steps` and `batch_size`. You should play with these numbers depending on how much time you have to spend waiting for training to complete.\n",
    "\n",
    "Reasonable numbers for `batch_size` include small powers of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train_steps = 20000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our _generator_ network is to take random noise as its input and to produce an image\n",
    "\n",
    "Our generator will have a very simple structure. The input will be a 100-dimensional noise vector $z$, and we'll use just two layers. As an equation, the network looks like:\n",
    "\n",
    "$G = \\sigma((\\mbox{relu}(z * W_1 + b_1)) * W_2 + b_2)$,\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is applied component-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "g_w1 = tf.get_variable(\"g_w1\", [100,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "g_b1 = tf.get_variable(\"g_b1\", [512], initializer=tf.constant_initializer(0.0))\n",
    "g_w2 = tf.get_variable(\"g_w2\", [512,784], initializer=tf.contrib.layers.xavier_initializer())\n",
    "g_b2 = tf.get_variable(\"g_b2\", [784], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "g_params = [g_w1, g_b1, g_w2, g_b2]\n",
    "\n",
    "def generator(z):\n",
    "    g_y1 = tf.nn.relu(tf.matmul(z, g_w1) + g_b1)\n",
    "    g = tf.matmul(g_y1, g_w2) + g_b2\n",
    "    G = tf.nn.sigmoid(g)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noise_prior(batch_size, dim):\n",
    "    return np.random.uniform(-1.0, 1.0, size=(batch_size, dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the discriminator is similar to that of the generator. The only difference is that instead of producing an entire image as output, the discriminator produces a single number that estimates the probability that the input $x$ came from the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "d_w1 = tf.get_variable(\"d_w1\", [784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "d_b1 = tf.get_variable(\"d_b1\", [256], initializer=tf.constant_initializer(0.0))\n",
    "d_w2 = tf.get_variable(\"d_w2\", [256,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "d_b2 = tf.get_variable(\"d_b2\", [1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "d_params = [d_w1, d_b1, d_w2, d_b2]\n",
    "\n",
    "def discriminator(x):\n",
    "    d_y1 = tf.nn.relu(tf.matmul(x, d_w1) + d_b1)\n",
    "    d_logit = tf.matmul(d_y1, d_w2) + d_b2\n",
    "    d_prob = tf.nn.sigmoid(d_logit)\n",
    "    return d_prob, d_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = generator(z)\n",
    "D_real, D_logit_real = discriminator(x)\n",
    "D_fake, D_logit_fake = discriminator(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an objective for $D$ and an objective for $G$. The objective for $D$ consists of a term that rewards correctly classifying actual data samples, and a term that rewards correctly picking out the fakes generated by $G$.\n",
    "\n",
    "The objective for $G$ is designed to reward it for correctly fooling $D$. We optimize both of these objectives using the same strategy and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj_d = -tf.reduce_mean(tf.log(D_real) + tf.log(1-D_fake))\n",
    "obj_g = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt_d = tf.train.AdamOptimizer(2e-5).minimize(obj_d, var_list=d_params)\n",
    "opt_g = tf.train.AdamOptimizer(2e-5).minimize(obj_g, var_list=g_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_d = 1\n",
    "k_g = 2\n",
    "histd, histg= np.zeros(n_train_steps), np.zeros(n_train_steps)\n",
    "for i in range(n_train_steps):\n",
    "    x_data, t_data = mnist.train.next_batch(batch_size)\n",
    "    for j in range(k_d):\n",
    "        noise = noise_prior(batch_size, 100)\n",
    "        histd[i], _ = sess.run([obj_d, opt_d], {x : x_data, z : noise})\n",
    "    for j in range(k_g):\n",
    "        noise = noise_prior(batch_size, 100)\n",
    "        histg[i], _ = sess.run([obj_g, opt_g], {z : noise})\n",
    "        \n",
    "    if i % (n_train_steps//10) == 0:\n",
    "        print i, histd[i], histg[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(n_train_steps), histd, label='obj_d')\n",
    "plt.plot(range(n_train_steps), histg, label='obj_g')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing The Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_im = sess.run(G, {z : noise_prior(1, 100)})\n",
    "out_im.shape = (28,28)\n",
    "imshow(out_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
